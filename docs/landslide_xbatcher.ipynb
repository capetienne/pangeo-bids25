{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a561757d-d091-44e4-b4cd-887f2a338196",
   "metadata": {},
   "source": [
    "# Rapid Landslide Detection using Synthetic Aperture Radar (SAR) Datacubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a408b-8a7c-47a4-9b2f-0b3eb138a3aa",
   "metadata": {},
   "source": [
    "## Authors & Contributors\n",
    "\n",
    "### Notebook\n",
    "- Alejandro Coca, The Alan Turing Institute (UK), [@acocac](https://github.com/acocac)\n",
    "\n",
    "### Contributors\n",
    "- Tina Odaka, Ifremer (France), [@tinaok](https://github.com/tinaok)\n",
    "\n",
    "### Modelling publications\n",
    "```{bibliography}\n",
    "  :style: plain\n",
    "  :list: bullet\n",
    "  :filter: topic % \"landslide\" and not topic % \"package\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6cea2-55ee-4f55-a79b-99b9076a6505",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> Overview\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>What is a datacube?</li>\n",
    "        <li>How do I generate a datacube?</li>\n",
    "        <li>How optimally train patch-wise deep learning models using the Pangeo stack?</li>\n",
    "        <li>What is U-Net model?</li>\n",
    "        <li>How can I use U-Net models for landslide detection?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about Earth Observation datacubes</li>\n",
    "        <li>Learn about U-Net models for landslide detection</li>\n",
    "        <li>Learn about xbatcher</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c18477-b688-4602-a63c-98d93cc220fd",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "With the increasing impact of climate change on landslide events, there is a need for rapid landslide detection technologies. Synthetic Aperture Radar (SAR) offers a solution by providing reliable measurements regardless of weather conditions. \n",
    "\n",
    "The advancement of deep learning (DL) algorithms for SAR applications is still stalled due to its intricate features and the need for extensive preprocessing. \n",
    "\n",
    "Earth Observation datacubes consist of processed data that can be easily utilized by researchers in remote sensing and machine learning fields. Remote sensing scientists can analyze time-series data of specific geographic areas, while machine learning experts can incorporate the data into neural network models in the form of tensors\n",
    "\n",
    "This notebook demonstrate the added value of analysis-ready SAR datasets to train supervised deep learning models for landslide detection.  \n",
    "\n",
    "We demonstrate the use of [Dask](https://docs.dask.org/) with [xbatcher](https://github.com/xarray-contrib/xbatcher) to parallelize the generation of the training and test partitions.\n",
    "\n",
    "### Modelling approach\n",
    "This notebook applies a U-Net architecture for landslide detection using SAR datacubes. The model is trained over an analysis-ready dataset comprising of SAR intensity and interferometry information, accumulated both before and after disaster events that initiated landslides.\n",
    "\n",
    "The details of the model are given in the paper entitled \"**Deep Learning for Rapid Landslide Detection using Synthetic Aperture Radar (SAR) Datacubes**\" {cite:ps}`a-boehm2022`.  \n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using the Hokkaido ARD dataset archived in [Zenodo](https://doi.org/10.5281/zenodo.7248056) and mirrored for the purpose of this notebook into a remote cloud storage bucket provided by [Pangeo-EOSC MinIO](https://github.com/pangeo-data/pangeo-eosc). The dataset contains multiple layers of SAR time-series data and interferometric products (coherence, InSAR) and auxiliary topographic information (elevation, slope, aspect, curvature) for multiple landslide events along with the corresponding landslide labels as indicated in the image below. \n",
    "\n",
    "```{figure} images/sar_datacube.png\n",
    ":name: sar_datacube\n",
    ":alt: Figure showing a datacube structure of dimensions (3 height x 3 width x 10 depth). Each slice represent a layer of the datacube containing Synthetic Aperture Radar (SAR) data and other geographic layers.\n",
    "\n",
    "Datacube containing Synthetic ApertureRadar (SAR) data and other geographic layers. Image credits: {cite:ps}`a-boehm2022`.\n",
    "```\n",
    "\n",
    "In addition to the Hokkaido dataset, there are other analysis-ready datacubes in three geographical areas: Puerto Rico,Mt Talakmau (Indonesia) and Kaikoura (New Zealand).\t\n",
    "\n",
    "All datasets are in [Zarr](https://zarr.dev) format and can be accessed locally through downloading the data from Zenodo or remotely through mirror datasets stored in a MinIO S3 compatible object storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdb0e4-b30f-43e4-8cbb-c523fa8290ea",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following main Python packages:\n",
    "\n",
    "- xarray {cite:ps}`a-xarray-hoyer2017` with [`zarr`](https://pypi.org/project/zarr/) engines\n",
    "- xbatcher {cite:ps}`a-xbatcher-2024`\n",
    "- pytorch {cite:ps}`a-pytorch-paszke2019` with pyTorch lightning{cite:ps}`a-plightning-falcon2019`\n",
    "- dask {cite:ps}`a-dask-2016`\n",
    "\n",
    "Please install these packages if not already available in your Python environment.\n",
    "\n",
    "### Packages\n",
    "\n",
    "In this episode, some Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d3ecc",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f222a5b852cd6d8",
   "metadata": {},
   "source": [
    "\"\"\"Math & Data Libraries\"\"\"\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xbatcher\n",
    "import s3fs\n",
    "import pooch\n",
    "from fsspec.implementations.zip import ZipFileSystem\n",
    "from fsspec.mapping import FSMap\n",
    "\n",
    "\"\"\" ML Libraries\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split\n",
    "from torchmetrics import AUROC, AveragePrecision, F1Score\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule, seed_everything\n",
    "\n",
    "\"\"\"Visualization Libraries\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\" Miscellaneous Libraries\"\"\"\n",
    "import copy \n",
    "from typing import Any, Optional, Tuple, List\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6d364830e67a71e",
   "metadata": {},
   "source": [
    "## Create a local Dask cluster on the local machine"
   ]
  },
  {
   "cell_type": "code",
   "id": "7cb030bf20380839",
   "metadata": {},
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()   # create `a local dask cluster on the local machine.\n",
    "client"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d138c951a4e2957",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618760ca2b8d30ba",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The experiments are defined using the `hparams` dictionary. We're setting the dataset name (`ds_name`), the dataset storage (`ds_storage`), the timestep length (`timestep_length`), the input variables (`input_vars`), the patch size (`patch_size`), the batch size (`batch_size`), the learning rate (`lr`), the weight decay (`weight_decay`), the loss function (`loss`), and the maximum number of epochs (`max_epochs`).\n",
    "\n",
    "In this experiment, we are working with the Hokkaido dataset loaded from a remote storage bucket. We are using SAR bands before and after the earthquake, plus DEM-derived data (elevation, slope, aspect, curvature) are used (total 8 input channels). We will only analyzing a single time step/satellite pass before and after the event. The patch size is set to 128x128 pixels, and the batch size is set to 64. The learning rate is set to 0.01, the weight decay to 0.0001, and the loss function to Cross-Entropy. The maximum number of epochs is set to 30.\n",
    "\n",
    "Further details of other experiments can be found in the methods paper {cite:ps}`a-boehm2022`. "
   ],
   "id": "46aff3719bc7e7e1"
  },
  {
   "cell_type": "code",
   "id": "a881eb85f3233d1e",
   "metadata": {},
   "source": [
    "hparams = {\n",
    "    \"ds_name\":\"hokkaido_japan\",\n",
    "    \"ds_storage\":\"remote\",\n",
    "    \"timestep_length\":1,\n",
    "    \"input_vars\": \"vv_before,vv_after,vh_before,vh_after,dem,dem_aspect,dem_curvature,dem_slope_radians\",\n",
    "    \"patch_size\": (128, 128),\n",
    "    \"batch_size\":64,\n",
    "    \"lr\":0.01,\n",
    "    \"weight_decay\":0.0001,\n",
    "    \"loss\":\"ce\",\n",
    "    \"max_epochs\":30\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fefeaf954fd05bab",
   "metadata": {},
   "source": [
    "### Data Module\n",
    "\n",
    "The functions and classes below, original defined in the modelling codebase, use `xarray` to access the dataset, `xbatcher` to facilitate the generation of patches and `torch.utils.data` modules to define datasets. \n",
    "\n",
    "In addition to the original code, we have added the `open_zarr_source` function to access the dataset from the original Zenodo source or the mirrored dataset stored in the MinIO S3 compatible object storage."
   ]
  },
  {
   "cell_type": "code",
   "id": "ee9c5c45d3280cb8",
   "metadata": {},
   "source": [
    "# function to access the Zarr dataset\n",
    "def open_zarr_source(ds_name, ds_storage):\n",
    "\n",
    "    if ds_storage == 'remote':\n",
    "        fst = s3fs.S3FileSystem(anon=True,\n",
    "          client_kwargs={\n",
    "             \"endpoint_url\": \"https://pangeo-eosc-minioapi.vm.fedcloud.eu/\"\n",
    "          })\n",
    "        \n",
    "        s3_prefix = 'acocacastro2'\n",
    "        s3_suffix = 'igarss24'\n",
    "        s3_bucket = s3_prefix + '-' + s3_suffix + '/'\n",
    "        \n",
    "        s3_path = \"s3://\" + s3_bucket + \"/\" + ds_name + \".zarr.zip\"\n",
    "        f = fst.open(s3_path)\n",
    "        fs = ZipFileSystem(f, mode=\"r\")\n",
    "        store = FSMap(\"\", fs, check=False)\n",
    "        ds = xr.open_zarr(store=store, consolidated=True)\n",
    "        \n",
    "    elif ds_storage == 'local':\n",
    "        pooch.retrieve(\n",
    "            url=\"doi:10.5281/zenodo.7248056/hokkaido_japan.zip\",\n",
    "            known_hash=\"md5:699b94e827c72bcd69bd786e56bfe5dc\",\n",
    "            processor=pooch.Unzip(extract_dir='data'),\n",
    "            path=f\".\",\n",
    "        )\n",
    "\n",
    "        ds = xr.open_zarr('data/' + ds_name + '.zarr')\n",
    "        \n",
    "    return ds\n",
    "\n",
    "# function to generate the before and after dataset using a given aggregation method using the time step length\n",
    "def before_after_ds(ds_path, ds_storage, ba_vars, aggregation, timestep_length, event_start_date, event_end_date):\n",
    "    ds = open_zarr_source(ds_path, ds_storage)\n",
    "    for var in ba_vars:\n",
    "        ds[var] = np.log(ds[var])\n",
    "    ds = ds.where(ds['sat:orbit_state'].compute() == 'd', drop=True)\n",
    "    before_ds = ds.drop_dims('timepair').sel(timestep=slice(None, event_start_date))\n",
    "    after_ds = ds.drop_dims('timepair').sel(timestep=slice(event_end_date, None))\n",
    "\n",
    "    if timestep_length < len(before_ds['timestep']):\n",
    "        before_ds = before_ds.isel(timestep=range(-1 - timestep_length, -1))\n",
    "\n",
    "    if timestep_length < len(after_ds['timestep']):\n",
    "        after_ds = after_ds.isel(timestep=range(timestep_length))\n",
    "\n",
    "    if aggregation == 'mean':\n",
    "        before_ds = before_ds.mean(dim=('timestep'))\n",
    "        after_ds = after_ds.mean(dim=('timestep'))\n",
    "    elif aggregation == 'median':\n",
    "        before_ds = before_ds.median(dim=('timestep'))\n",
    "        after_ds = after_ds.median(dim=('timestep'))\n",
    "\n",
    "    before_after_vars = []\n",
    "    for suffix in ['before', 'after']:\n",
    "        for var in ba_vars:\n",
    "            before_after_vars.append(f'{var}_{suffix}')\n",
    "    the_ds = before_ds.rename_vars({var: f'{var}_before' for var in ba_vars})\n",
    "    for var in ba_vars:\n",
    "        the_ds[f'{var}_after'] = after_ds[var]\n",
    "    for var in the_ds.data_vars:\n",
    "        the_ds[f'{var}_mean'] = the_ds[var].mean()\n",
    "        the_ds[f'{var}_std'] = the_ds[var].std()\n",
    "    return the_ds.compute().load()\n",
    "\n",
    "# function to generate batches of data using xbatcher\n",
    "def batching_dataset(ds, patch_size, input_vars, target, include_negatives):\n",
    "    mean_std_dict = {}\n",
    "    for var in input_vars:\n",
    "        if not mean_std_dict.get(var):\n",
    "            mean_std_dict[var] = {}\n",
    "        mean_std_dict[var]['mean'] = ds[f'{var}_mean'].values\n",
    "        mean_std_dict[var]['std'] = ds[f'{var}_std'].values\n",
    "\n",
    "    batches = []\n",
    "    bgen = xbatcher.BatchGenerator(ds, {'x': patch_size[0], 'y': patch_size[1]})\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    for batch in bgen:\n",
    "        positives_tmp = batch[target].sum().item()\n",
    "        if not include_negatives and positives_tmp > 0:\n",
    "            positives = positives + positives_tmp\n",
    "            negatives += batch[target].size\n",
    "            batches.append(batch)\n",
    "        elif include_negatives and (batch['dem'] <= 0).sum() == 0:\n",
    "            positives = positives + positives_tmp\n",
    "            negatives += batch[target].size\n",
    "            batches.append(batch)\n",
    "    print(f\"P/(P+N)\", positives / negatives)\n",
    "    return batches, mean_std_dict\n",
    "\n",
    "# class using the pytorch Dataset module to define the dataset\n",
    "class BeforeAfterDatasetBatches(Dataset):\n",
    "    def __init__(self, batches, input_vars, target, mean_std_dict=None):\n",
    "        print(\"**************** INIT CALLED ******************\")\n",
    "        self.batches = batches\n",
    "        self.target = target\n",
    "        self.input_vars = input_vars\n",
    "        self.mean = np.stack([mean_std_dict[var]['mean'] for var in input_vars]).reshape((-1, 1, 1))\n",
    "        self.std = np.stack([mean_std_dict[var]['std'] for var in input_vars]).reshape((-1, 1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.batches[idx]\n",
    "        inputs = np.stack([batch[var].values for var in self.input_vars])\n",
    "        inputs = (inputs - self.mean) / self.std\n",
    "\n",
    "        target = batch[self.target].values\n",
    "        inputs = np.nan_to_num(inputs, nan=0)\n",
    "        target = np.nan_to_num(target, nan=0)\n",
    "        target = (target > 0)\n",
    "        return inputs, target\n",
    "\n",
    "# class using the lightning DataModule to define the data module implementing five key methods/steps\n",
    "class BeforeAfterCubeDataModule(LightningDataModule):\n",
    "    \"\"\"LightningDataModule.\n",
    "\n",
    "    A DataModule implements 5 key methods:\n",
    "        - prepare_data (things to do on 1 GPU/TPU, not on every GPU/TPU in distributed mode)\n",
    "        - setup (things to do on every accelerator in distributed mode)\n",
    "        - train_dataloader (the training dataloader)\n",
    "        - val_dataloader (the validation dataloader(s))\n",
    "        - test_dataloader (the test dataloader(s))\n",
    "\n",
    "    This allows you to share a full dataset without explaining how to download,\n",
    "    split, transform and process the data.\n",
    "\n",
    "    Read the docs:\n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            ds_name: str,\n",
    "            ds_storage: str,\n",
    "            ba_vars,\n",
    "            aggregation,\n",
    "            timestep_length,\n",
    "            event_start_date,\n",
    "            event_end_date,\n",
    "            input_vars,\n",
    "            target,\n",
    "            include_negatives=False,\n",
    "            train_val_test_split: Tuple[float, float, float] = (0.7, 0.2, 0.1),\n",
    "            patch_size: Tuple[int, int] = (128, 128),\n",
    "            batch_size: int = 64,\n",
    "            num_workers: int = 0,\n",
    "            pin_memory: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.ds = None\n",
    "        self.batches = None\n",
    "        self.mean_std_dict = None\n",
    "\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "        self.data_whole: Optional[Dataset] = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n",
    "\n",
    "        This method is called by lightning when doing `trainer.fit()` and `trainer.test()`,\n",
    "        so be careful not to execute the random split twice! The `stage` can be used to\n",
    "        differentiate whether it's called before trainer.fit()` or `trainer.test()`.\n",
    "        \"\"\"\n",
    "        # load datasets only if they're not loaded already\n",
    "        if not self.data_train and not self.data_val and not self.data_test:\n",
    "            self.ds = before_after_ds(self.hparams.ds_name, self.hparams.ds_storage, self.hparams.ba_vars, self.hparams.aggregation,\n",
    "                                      self.hparams.timestep_length, self.hparams.event_start_date,\n",
    "                                      self.hparams.event_end_date)\n",
    "            self.batches, self.mean_std_dict = batching_dataset(self.ds, self.hparams.patch_size, self.hparams.input_vars, self.hparams.target,\n",
    "                                                                self.hparams.include_negatives)\n",
    "\n",
    "            dataset = BeforeAfterDatasetBatches(self.batches, self.hparams.input_vars, self.hparams.target,\n",
    "                                                        mean_std_dict=self.mean_std_dict)\n",
    "\n",
    "            self.data_whole = dataset\n",
    "\n",
    "            if self.hparams.train_val_test_split:\n",
    "                train_val_test_split = [int(len(dataset) * x) for x in self.hparams.train_val_test_split]\n",
    "                train_val_test_split[2] = len(dataset) - train_val_test_split[1] - train_val_test_split[0]\n",
    "                train_val_test_split = tuple(train_val_test_split)\n",
    "                print(\"*\" * 20)\n",
    "                print(\"Train - Val - Test SPLIT\", train_val_test_split)\n",
    "                print(\"*\" * 20)\n",
    "                self.data_train, self.data_val, self.data_test = random_split(\n",
    "                    dataset=dataset,\n",
    "                    lengths=train_val_test_split,\n",
    "                    generator=torch.Generator().manual_seed(42),\n",
    "                )\n",
    "    \n",
    "                print(\"*\" * 20)\n",
    "                print(\"Train - Val - Test LENGTHS\", len(self.data_train), len(self.data_val), len(self.data_test))\n",
    "                print(\"*\" * 20)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return MultiEpochsDataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=True,\n",
    "            persistent_workers=(self.hparams.num_workers > 0)\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return MultiEpochsDataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "            persistent_workers=(self.hparams.num_workers > 0)\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return MultiEpochsDataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "            persistent_workers=(self.hparams.num_workers > 0)\n",
    "        )\n",
    "\n",
    "class MultiEpochsDataLoader(torch.utils.data.DataLoader):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._DataLoader__initialized = False\n",
    "        self.batch_sampler = _RepeatSampler(self.batch_sampler)\n",
    "        self._DataLoader__initialized = True\n",
    "        self.iterator = super().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler.sampler)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield next(self.iterator)\n",
    "\n",
    "\n",
    "class _RepeatSampler(object):\n",
    "    \"\"\" Sampler that repeats forever.\n",
    "    Args:\n",
    "        sampler (Sampler)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield from iter(self.sampler)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dca3717bbff9d009",
   "metadata": {},
   "source": [
    "### U-Net Model\n",
    "\n",
    "The class below defines the U-Net architecture using `pl.LightningModule`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ":::{note}\n",
    "\n",
    "The U-Net model {cite:ps}`a-unet_ronneberger15`.  is a convolutional neural network that was originally developed for biomedical image segmentation and extended to other applications. U-Net has an encoder-decoder architecture with skip connections that allow the model to capture both local and global features. The model is composed of a contracting path that captures context and a symmetric expanding path that enables precise localization.\n",
    "\n",
    "For the landslide example, a RestNet50 (48,982,754 trainable parameters)  is used as the encoder in the U-Net model. The model is trained using the Dice loss function and evaluated using the Area Under the Receiver Operating Characteristic (AUROC), Average Precision (AUPRC), and F1 score metrics.\n",
    "\n",
    ":::"
   ],
   "id": "ac0b694fdb602d04"
  },
  {
   "cell_type": "code",
   "id": "5b212177329a1370",
   "metadata": {},
   "source": [
    "class plUNET(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            lr: float = 0.001,\n",
    "            weight_decay: float = 0.0005,\n",
    "            num_channels: int = 1,\n",
    "            loss='dice'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.net = smp.UnetPlusPlus(encoder_name='resnet50', in_channels=num_channels, classes=2)\n",
    "\n",
    "        if loss == 'dice':\n",
    "            self.criterion = smp.losses.DiceLoss(mode='multiclass')\n",
    "        elif loss == 'ce':\n",
    "            self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_auc = AUROC(task=\"binary\")\n",
    "        self.train_f1 = F1Score(task=\"binary\")\n",
    "        self.train_auprc = AveragePrecision(task=\"binary\")\n",
    "        self.val_auc = AUROC(task=\"binary\")\n",
    "        self.val_f1 = F1Score(task=\"binary\")\n",
    "        self.val_auprc = AveragePrecision(task=\"binary\")\n",
    "        self.test_auc = AUROC(task=\"binary\")\n",
    "        self.test_auprc = AveragePrecision(task=\"binary\")\n",
    "        self.test_f1 = F1Score(task=\"binary\")\n",
    "\n",
    "        self.train_positive_count = 0\n",
    "        self.val_positive_count = 0\n",
    "        self.test_positive_count = 0\n",
    "        self.train_negative_count = 0\n",
    "        self.val_negative_count = 0\n",
    "        self.test_negative_count = 0\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "    def step(self, batch: Any):\n",
    "        x, y = batch\n",
    "        y = y.long()\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        return loss, preds, y, x\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets, inputs = self.step(batch)\n",
    "        self.train_auc.update(preds, targets)\n",
    "        self.train_auprc.update(preds.flatten(), targets.flatten())\n",
    "        self.train_f1.update(preds.flatten(), targets.flatten())\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/auc\", self.train_auc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/auprc\", self.train_auprc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/f1\", self.train_f1, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets, \"inputs\": inputs}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets, _ = self.step(batch)\n",
    "\n",
    "        # log val metrics\n",
    "        self.val_auc.update(preds, targets)\n",
    "        self.val_auprc.update(preds.flatten(), targets.flatten())\n",
    "        self.val_f1.update(preds.flatten(), targets.flatten())\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/auc\", self.val_auc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/auprc\", self.val_auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/f1\", self.val_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets, _ = self.step(batch)\n",
    "        self.test_auc.update(preds, targets)\n",
    "        self.test_auprc.update(preds.flatten(), targets.flatten())\n",
    "        self.test_f1.update(preds.flatten(), targets.flatten())\n",
    "\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test/auc\", self.test_auc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"test/auprc\", self.test_auprc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"test/f1\", self.test_f1, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler, \"monitor\": \"train/loss\"}\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.load_state_dict(torch.load(filename))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3956c11c",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "The pipeline consists in defining and initiating the datacube, defining and initiating the U-Net model, training the model, saving the trained model. Finally, we evaluate the model using the test dataset and visualize the results over one of the test batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c943d3-c3fe-423c-908e-2e3da2e53bf4",
   "metadata": {},
   "source": [
    "### Set a seed ([reproducibility](https://lightning.ai/docs/pytorch/stable/common/trainer.html#reproducibility))"
   ]
  },
  {
   "cell_type": "code",
   "id": "81af26fc-2c8c-4bfd-89f7-4e8b72c05094",
   "metadata": {},
   "source": [
    "seed_everything(42, workers=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec43e9a6ea1f055b",
   "metadata": {},
   "source": [
    "### Define the datacube"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3563fca-ba71-4ff2-8a84-7d9fa7dcc9ca",
   "metadata": {},
   "source": [
    "hparams[\"input_vars\"] = hparams[\"input_vars\"].split(',')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2013c638dc28b5c9",
   "metadata": {},
   "source": [
    "dm = BeforeAfterCubeDataModule(\n",
    "    ds_name=hparams[\"ds_name\"],\n",
    "    ds_storage=hparams[\"ds_storage\"],\n",
    "    ba_vars=['vv', 'vh'],\n",
    "    aggregation='mean',\n",
    "    timestep_length=hparams[\"timestep_length\"],\n",
    "    event_start_date='20180905',\n",
    "    event_end_date='20180907',\n",
    "    input_vars=hparams[\"input_vars\"],\n",
    "    target='landslides',\n",
    "    train_val_test_split=(0.7, 0.2, 0.1),\n",
    "    patch_size=hparams[\"patch_size\"],\n",
    "    batch_size=hparams[\"batch_size\"],\n",
    "    include_negatives=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c04e5f767e67664b",
   "metadata": {},
   "source": [
    "### Initiate the datacube"
   ]
  },
  {
   "cell_type": "code",
   "id": "90e38ad745b14732",
   "metadata": {},
   "source": [
    "dm.setup()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6921d51be43de425",
   "metadata": {},
   "source": [
    "## U-Net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca623f6872a0b4b3",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcd588eb01c432",
   "metadata": {},
   "source": [
    "We start defining the model using the `plUNET` class and the hyperparameters defined in the `hparams` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "id": "2e265ae6130f54ac",
   "metadata": {},
   "source": [
    "model = plUNET(\n",
    "        lr=hparams[\"lr\"],\n",
    "        weight_decay=hparams[\"weight_decay\"],\n",
    "        num_channels=len(hparams[\"input_vars\"]),\n",
    "        loss=hparams[\"loss\"]\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "89934db9bca074d8",
   "metadata": {},
   "source": [
    "Then we initiate the trainer with the maximum number of epochs defined in the `hparams` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "id": "5c99c8eb1061be64",
   "metadata": {},
   "source": [
    "trainer = pl.Trainer(max_epochs=hparams[\"max_epochs\"], accelerator=\"auto\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6be36a24aad32710",
   "metadata": {},
   "source": [
    "Finally, we fit the trainer with the model and the datacube."
   ]
  },
  {
   "cell_type": "code",
   "id": "9cd183cef5ecc8c6",
   "metadata": {},
   "source": [
    "trainer.fit(model, dm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b72fbae-177f-4882-949b-11419aa86920",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "id": "236bbb86-852c-426d-b43d-3cfc9aa67124",
   "metadata": {},
   "source": [
    "model.save(\"landslide.pt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e38650c",
   "metadata": {},
   "source": "### Model Evaluation"
  },
  {
   "cell_type": "markdown",
   "id": "2a9634e1-8dc5-41db-b864-b3ea430d49fd",
   "metadata": {},
   "source": [
    "We assess the model performance using the test dataset and the best model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "id": "e3320d68-3d6b-402b-b674-64089c7a7525",
   "metadata": {},
   "source": [
    "result = trainer.test(ckpt_path=\"best\", datamodule=dm)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e0b2ecd-eed4-437a-b27c-8850791fc2ea",
   "metadata": {},
   "source": [
    "def vizresults(inputs, prediction, target):\n",
    "    npinputs = inputs.detach().cpu().numpy()\n",
    "    npprediction = prediction.detach().cpu().numpy()\n",
    "    nptarget = target.detach().cpu().numpy()\n",
    "\n",
    "    f, axarr = plt.subplots(1,10, figsize=(20,5))\n",
    "    axarr[0].imshow(npprediction)\n",
    "    axarr[1].imshow(nptarget)\n",
    "\n",
    "    for i in range(len(hparams[\"input_vars\"])):\n",
    "        axarr[i+2].imshow(npinputs[i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5e6434c-92db-452c-b50c-6eca59698f32",
   "metadata": {},
   "source": [
    "# get some random training images\n",
    "dataiter = iter(dm.test_dataloader())\n",
    "\n",
    "images, target = next(dataiter)\n",
    "prediction = model(images).argmax(dim=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80f9b2c0-ea60-4c50-a3c2-3457cf37da6f",
   "metadata": {},
   "source": [
    "batchid = 0\n",
    "vizresults(images[batchid],target[batchid],prediction[batchid])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2fbb4b57-0b4f-4c2e-af55-596a42f0fab9",
   "metadata": {},
   "source": "## Prediction over larger batches"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fully convolutional networks like U-Net do not have fixed input shape, so we could try a different size input for the trained model. We demonstrate how to load the trained model and evaluate it over a larger batch of 1024 pixels x 1024 pixels of data.",
   "id": "7d643cb1a62e9def"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load the trained model",
   "id": "e12bbc4e7686136c"
  },
  {
   "cell_type": "code",
   "id": "a00a4c07-22c8-4100-8d13-354703462aba",
   "metadata": {},
   "source": [
    "path = 'landslide.pt'\n",
    "model = plUNET(\n",
    "        num_channels=len(hparams[\"input_vars\"]))\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Set the experiment\n",
    "\n",
    "We copy the original hyperparameters and change the patch size to 1024x1024 pixels and the batch size to 1."
   ],
   "id": "cf4b58d082bbe38f"
  },
  {
   "cell_type": "code",
   "id": "0d250e61-ee1d-49ea-a649-0cfc26324eed",
   "metadata": {},
   "source": [
    "hparams_all = copy.deepcopy(hparams)\n",
    "hparams_all['patch_size']=[1024,1024]\n",
    "hparams_all['batch_size']=1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The original data module is initiated data that discarded batches with negative values (no landslides). We set `include_negatives=True` to include all batches and predict over the whole surface.",
   "id": "f70870b7eed16ad1"
  },
  {
   "cell_type": "code",
   "id": "3fae3043-ac8b-4723-a1f3-d4db91755a46",
   "metadata": {},
   "source": [
    "dm_all= BeforeAfterCubeDataModule(\n",
    "    ds_name=hparams[\"ds_name\"],\n",
    "    ds_storage=hparams[\"ds_storage\"],\n",
    "    ba_vars=['vv', 'vh'],\n",
    "    aggregation='mean',\n",
    "    timestep_length=hparams[\"timestep_length\"],\n",
    "    event_start_date='20180905',\n",
    "    event_end_date='20180907',\n",
    "    input_vars=hparams[\"input_vars\"],\n",
    "    target='landslides',\n",
    "    train_val_test_split=None,\n",
    "    patch_size=hparams_all[\"patch_size\"],\n",
    "    batch_size=hparams_all[\"batch_size\"],\n",
    "    include_negatives=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a675707-02ca-4200-ad85-fe69c22f3f82",
   "metadata": {},
   "source": [
    "dm_all.setup()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We set a dataloader to load the whole dataset.",
   "id": "a0281f53cd7bd273"
  },
  {
   "cell_type": "code",
   "id": "4d43e52e-dae7-4b19-95ee-96d15699a58c",
   "metadata": {},
   "source": [
    "ds = dm_all.data_whole\n",
    "dataloader = torch.utils.data.DataLoader(ds, pin_memory=False, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model inference",
   "id": "1fa2449cb63bb553"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We run the model inference over the whole dataset and visualise over a batch of 1024 pixels x 1024 pixels.",
   "id": "433d8441271cb586"
  },
  {
   "cell_type": "code",
   "id": "f1337768-929e-45d3-b36e-4e49cdc28145",
   "metadata": {},
   "source": [
    "targets = []\n",
    "outputs = []\n",
    "for batch_idx, (image, target) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    output = model(image)\n",
    "    outputs.append(output[:, 0].cpu().detach().numpy())\n",
    "    targets.append(target.cpu().detach().numpy())\n",
    "    \n",
    "targets = np.concatenate(targets, axis=0)\n",
    "outputs = np.concatenate(outputs, axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5aa22e1f-d0b3-41dd-8ffc-6401dd796781",
   "metadata": {},
   "source": [
    "batch_id = 6\n",
    "f, axarr = plt.subplots(1,2, figsize=(20,5))\n",
    "axarr[0].imshow(targets[batch_id])\n",
    "axarr[1].imshow(outputs[batch_id]<0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ":::{warning}\n",
    "\n",
    "We could reshape the prediction numpy arrays (flattened) according to the dimensions of the original xarray dataset. However, this step is not included in this notebook because xbatcher [drops partial batches](https://github.com/xarray-contrib/xbatcher/discussions/82), and the output prediction will not match the original dataset.\n",
    "\n",
    "```python\n",
    "input_ = outputs.flatten() #flatten the outputs\n",
    "input_ds = dm_all.ds #extract the xarray dataset\n",
    "len_x, len_y = len(input_ds['x']), len(input_ds['y']) #extract the length of the x and y dimensions\n",
    "y = input_.reshape(len_y, len_x) #reshape the outputs\n",
    "````\n",
    "\n",
    "Workarounds include padding the original xarray dataset to match a divisor of the batch size or using a different library to generate the batches. \n",
    "\n",
    "Ideas or suggestions are welcome in the [xbatcher repository](https://github.com/xarray-contrib/xbatcher).\n",
    "\n",
    ":::"
   ],
   "id": "d04db5d457e972dc"
  },
  {
   "cell_type": "markdown",
   "id": "f1475fba-d335-48cc-8566-5370a0169a41",
   "metadata": {},
   "source": [
    " ## References\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"landslide\" and not topic % \"package\"\n",
    ":keyprefix: a-\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf3c7c-6684-4edf-99cc-e30016b2d4a8",
   "metadata": {},
   "source": [
    "## Packages citation\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"landslide\" and topic % \"package\"\n",
    ":keyprefix: a-\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
