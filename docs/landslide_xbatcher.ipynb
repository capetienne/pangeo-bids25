{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a561757d-d091-44e4-b4cd-887f2a338196",
   "metadata": {},
   "source": [
    "# Rapid Landslide Detection using Synthetic Aperture Radar (SAR) Datacubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a408b-8a7c-47a4-9b2f-0b3eb138a3aa",
   "metadata": {},
   "source": [
    "## Authors & Contributors\n",
    "### Notebook\n",
    "- Alejandro Coca, The Alan Turing Institute (France), [@acocac](https://github.com/acocac)\n",
    "\n",
    "### Contributors\n",
    "- Tina Odaka, Ifremer (France), [@tinaok](https://github.com/tinaok)\n",
    "\n",
    "### Modelling publication\n",
    "```{bibliography}\n",
    "  :style: plain\n",
    "  :list: bullet\n",
    "  :filter: topic % \"boehm2022\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6cea2-55ee-4f55-a79b-99b9076a6505",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<i class=\"fa-question-circle fa\" style=\"font-size: 22px;color:#666;\"></i> Overview\n",
    "    <br>\n",
    "    <br>\n",
    "    <b>Questions</b>\n",
    "    <ul>\n",
    "        <li>What is a datacube?</li>\n",
    "        <li>How do I generate a datacube?</li>\n",
    "        <li>How optimally train patch-wise deep learning models using the Pangeo stack?</li>\n",
    "        <li>What is U-Net model?</li>\n",
    "        <li>How can I use U-Net models for landslide detection?</li>\n",
    "    </ul>\n",
    "    <b>Objectives</b>\n",
    "    <ul>\n",
    "        <li>Learn about Earth Observation datacubes</li>\n",
    "        <li>Learn about U-Net models for landslide detection</li>\n",
    "        <li>Learn about xbatcher</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c18477-b688-4602-a63c-98d93cc220fd",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "With the increasing impact of climate change on landslide events, there is a need for rapid landslide detection technologies. Synthetic Aperture Radar (SAR) offers a solution by providing reliable measurements regardless of weather conditions. \n",
    "\n",
    "The advancement of deep learning (DL) algorithms for SAR applications is still stalled due to its intricate features and the need for extensive preprocessing. \n",
    "\n",
    "Earth Observation datacubes consist of processed data that can be easily utilized by researchers in remote sensing and machine learning fields. Remote sensing scientists can analyze time-series data of specific geographic areas, while machine learning experts can incorporate the data into neural network models in the form of tensors\n",
    "\n",
    "This notebook demonstrate the added value of analysis-ready SAR datasets to train supervised deep learning models for landslide detection.  \n",
    "\n",
    "We demonstrate the use of [Dask](https://docs.dask.org/) with [xbatcher](https://github.com/xarray-contrib/xbatcher) to parallelize the generation of the training and test partitions.\n",
    "\n",
    "### Modelling approach\n",
    "This notebook applies a U-Net architecture for landslide detection using SAR datacubes. The model is trained over an analysis-ready dataset comprising of SAR intensity and interferometry information, accumulated both before and after disaster events that initiated landslides.\n",
    "\n",
    "The details of the model are given in the paper entitled \"**Deep Learning for Rapid Landslide Detection using Synthetic Aperture Radar (SAR) Datacubes**\" {cite:ps}`a-boehm2022`  \n",
    "\n",
    "### Data\n",
    "\n",
    "In this episode, we will be using the Hokkaido ARD dataset archived in [Zenodo](https://doi.org/10.5281/zenodo.7248056). The dataset contains multiple layers of SAR time-series data and interferometric products (coherence, InSAR) and auxiliary topographic information (elevation, slope, aspect, curvature) for multiple landslide events along with the corresponding landslide labels as indicated in the image below. \n",
    "\n",
    "![SAR Data Cube](images/sar_datacube.png)\n",
    "\n",
    "In addition to the Hokkaido dataset, there are other analysis-ready datacubes in three geographical areas: Puerto Rico,Mt Talakmau (Indonesia) and Kaikoura (New Zealand).\t\n",
    "\n",
    "All datasets are in [Zarr](https://zarr.dev) format and can be accessed locally through downloading the data from Zenodo or remotely through mirror datasets stored in a MinIO S3 compatible object storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdb0e4-b30f-43e4-8cbb-c523fa8290ea",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This episode uses the following main Python packages:\n",
    "\n",
    "- numpy {cite:ps}`a-numpy-harris2020`   \n",
    "- xarray {cite:ps}`a-xarray-hoyer2017` with [`zarr`](https://pypi.org/project/zarr/) engines\n",
    "- xbatcher {cite:ps}`a-xbatcher-2024`\n",
    "\n",
    "Please install these packages if not already available in your Python environment.\n",
    "\n",
    "### Packages\n",
    "\n",
    "In this episode, Python packages are imported when we start to use them. However, for best software practices, we recommend you to install and import all the necessary libraries at the top of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d3ecc",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f222a5b852cd6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Math & Data Libraries\"\"\"\n",
    "import numpy as np\n",
    "import pooch\n",
    "import xarray as xr\n",
    "import xbatcher\n",
    "\n",
    "\"\"\" ML Libraries\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split\n",
    "from torchmetrics import AUROC, AveragePrecision, F1Score\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "\"\"\"Visualization Libraries\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\" Miscellaneous Libraries\"\"\"\n",
    "from typing import Any, Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d364830e67a71e",
   "metadata": {},
   "source": [
    "## Create a local Dask cluster on the local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb030bf20380839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()   # create a local dask cluster on the local machine.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138c951a4e2957",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618760ca2b8d30ba",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a881eb85f3233d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"ds_path\":\"data/hokkaido_japan.zarr\",\n",
    "    \"timestep_length\":1,\n",
    "    \"input_vars\": \"vv_before,vv_after,vh_before,vh_after,dem,dem_aspect,dem_curvature,dem_slope_radians\",\n",
    "    \"batch_size\":64,\n",
    "    \"lr\":0.01,\n",
    "    \"weight_decay\":0.0001,\n",
    "    \"loss\":\"ce\",\n",
    "    \"max_epochs\":30\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefeaf954fd05bab",
   "metadata": {},
   "source": [
    "### Data Module\n",
    "\n",
    "The functions and classes defined below use `xarray` to access the dataset, `xbatcher` to facilitate the generation of patches and `torch.utils.data` modules to define datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c5c45d3280cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_after_ds(ds_path, ba_vars, aggregation, timestep_length, event_start_date, event_end_date):\n",
    "    ds = xr.open_zarr(ds_path)\n",
    "    for var in ba_vars:\n",
    "        ds[var] = np.log(ds[var])\n",
    "    ds = ds.where(ds['sat:orbit_state'].compute() == 'd', drop=True)\n",
    "    before_ds = ds.drop_dims('timepair').sel(timestep=slice(None, event_start_date))\n",
    "    after_ds = ds.drop_dims('timepair').sel(timestep=slice(event_end_date, None))\n",
    "\n",
    "    if timestep_length < len(before_ds['timestep']):\n",
    "        before_ds = before_ds.isel(timestep=range(-1 - timestep_length, -1))\n",
    "\n",
    "    if timestep_length < len(after_ds['timestep']):\n",
    "        after_ds = after_ds.isel(timestep=range(timestep_length))\n",
    "\n",
    "    if aggregation == 'mean':\n",
    "        before_ds = before_ds.mean(dim=('timestep'))\n",
    "        after_ds = after_ds.mean(dim=('timestep'))\n",
    "    elif aggregation == 'median':\n",
    "        before_ds = before_ds.median(dim=('timestep'))\n",
    "        after_ds = after_ds.median(dim=('timestep'))\n",
    "\n",
    "    before_after_vars = []\n",
    "    for suffix in ['before', 'after']:\n",
    "        for var in ba_vars:\n",
    "            before_after_vars.append(f'{var}_{suffix}')\n",
    "    the_ds = before_ds.rename_vars({var: f'{var}_before' for var in ba_vars})\n",
    "    for var in ba_vars:\n",
    "        the_ds[f'{var}_after'] = after_ds[var]\n",
    "    for var in the_ds.data_vars:\n",
    "        the_ds[f'{var}_mean'] = the_ds[var].mean()\n",
    "        the_ds[f'{var}_std'] = the_ds[var].std()\n",
    "    return the_ds.compute().load()\n",
    "\n",
    "\n",
    "def batching_dataset(ds, input_vars, target, include_negatives):\n",
    "    mean_std_dict = {}\n",
    "    for var in input_vars:\n",
    "        if not mean_std_dict.get(var):\n",
    "            mean_std_dict[var] = {}\n",
    "        mean_std_dict[var]['mean'] = ds[f'{var}_mean'].values\n",
    "        mean_std_dict[var]['std'] = ds[f'{var}_std'].values\n",
    "\n",
    "    batches = []\n",
    "    bgen = xbatcher.BatchGenerator(ds, {'x': 128, 'y': 128})\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    for batch in bgen:\n",
    "        positives_tmp = batch[target].sum().item()\n",
    "        if not include_negatives and positives_tmp > 0:\n",
    "            positives = positives + positives_tmp\n",
    "            negatives += batch[target].size\n",
    "            batches.append(batch)\n",
    "        elif include_negatives and (batch['dem'] <= 0).sum() == 0:\n",
    "            positives = positives + positives_tmp\n",
    "            negatives += batch[target].size\n",
    "            batches.append(batch)\n",
    "    print(f\"P/(P+N)\", positives / negatives)\n",
    "    return batches, mean_std_dict\n",
    "\n",
    "\n",
    "class BeforeAfterDatasetBatches(Dataset):\n",
    "    def __init__(self, batches, input_vars, target, mean_std_dict=None):\n",
    "        print(\"**************** INIT CALLED ******************\")\n",
    "        self.batches = batches\n",
    "        self.target = target\n",
    "        self.input_vars = input_vars\n",
    "        self.mean = np.stack([mean_std_dict[var]['mean'] for var in input_vars]).reshape((-1, 1, 1))\n",
    "        self.std = np.stack([mean_std_dict[var]['std'] for var in input_vars]).reshape((-1, 1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.batches[idx]\n",
    "        inputs = np.stack([batch[var].values for var in self.input_vars])\n",
    "        inputs = (inputs - self.mean) / self.std\n",
    "\n",
    "        target = batch[self.target].values\n",
    "        inputs = np.nan_to_num(inputs, nan=0)\n",
    "        target = np.nan_to_num(target, nan=0)\n",
    "        target = (target > 0)\n",
    "        return inputs, target\n",
    "\n",
    "\n",
    "class BeforeAfterCubeDataModule(LightningDataModule):\n",
    "    \"\"\"LightningDataModule.\n",
    "\n",
    "    A DataModule implements 5 key methods:\n",
    "        - prepare_data (things to do on 1 GPU/TPU, not on every GPU/TPU in distributed mode)\n",
    "        - setup (things to do on every accelerator in distributed mode)\n",
    "        - train_dataloader (the training dataloader)\n",
    "        - val_dataloader (the validation dataloader(s))\n",
    "        - test_dataloader (the test dataloader(s))\n",
    "\n",
    "    This allows you to share a full dataset without explaining how to download,\n",
    "    split, transform and process the data.\n",
    "\n",
    "    Read the docs:\n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            ds_path: str,\n",
    "            ba_vars,\n",
    "            aggregation,\n",
    "            timestep_length,\n",
    "            event_start_date,\n",
    "            event_end_date,\n",
    "            input_vars,\n",
    "            target,\n",
    "            include_negatives=False,\n",
    "            train_val_test_split: Tuple[float, float, float] = (0.7, 0.2, 0.1),\n",
    "            batch_size: int = 64,\n",
    "            num_workers: int = 0,\n",
    "            pin_memory: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.ds = None\n",
    "        self.batches = None\n",
    "        self.mean_std_dict = None\n",
    "\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n",
    "\n",
    "        This method is called by lightning when doing `trainer.fit()` and `trainer.test()`,\n",
    "        so be careful not to execute the random split twice! The `stage` can be used to\n",
    "        differentiate whether it's called before trainer.fit()` or `trainer.test()`.\n",
    "        \"\"\"\n",
    "        # load datasets only if they're not loaded already\n",
    "        if not self.data_train and not self.data_val and not self.data_test:\n",
    "            self.ds = before_after_ds(self.hparams.ds_path, self.hparams.ba_vars, self.hparams.aggregation,\n",
    "                                      self.hparams.timestep_length, self.hparams.event_start_date,\n",
    "                                      self.hparams.event_end_date)\n",
    "            self.batches, self.mean_std_dict = batching_dataset(self.ds, self.hparams.input_vars, self.hparams.target,\n",
    "                                                                self.hparams.include_negatives)\n",
    "\n",
    "            dataset = BeforeAfterDatasetBatches(self.batches, self.hparams.input_vars, self.hparams.target,\n",
    "                                                        mean_std_dict=self.mean_std_dict)\n",
    "            # self.data_val = BeforeAfterDatasetBatches(self.batches, self.hparams.input_vars, self.hparams.target,\n",
    "            #                                           mean_std_dict=self.mean_std_dict,\n",
    "            #                                           )\n",
    "            # self.data_test = BeforeAfterDatasetBatches(self.batches, self.hparams.input_vars, self.hparams.target,\n",
    "            #                                            mean_std_dict=self.mean_std_dict,\n",
    "            #                                            )\n",
    "\n",
    "            train_val_test_split = [int(len(dataset) * x) for x in self.hparams.train_val_test_split]\n",
    "            train_val_test_split[2] = len(dataset) - train_val_test_split[1] - train_val_test_split[0]\n",
    "            train_val_test_split = tuple(train_val_test_split)\n",
    "            print(\"*\" * 20)\n",
    "            print(\"Train - Val - Test SPLIT\", train_val_test_split)\n",
    "            print(\"*\" * 20)\n",
    "            self.data_train, self.data_val, self.data_test = random_split(\n",
    "                dataset=dataset,\n",
    "                lengths=train_val_test_split,\n",
    "                generator=torch.Generator().manual_seed(42),\n",
    "            )\n",
    "\n",
    "            print(\"*\" * 20)\n",
    "            print(\"Train - Val - Test LENGTHS\", len(self.data_train), len(self.data_val), len(self.data_test))\n",
    "            print(\"*\" * 20)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return MultiEpochsDataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=True,\n",
    "            persistent_workers=(self.hparams.num_workers > 0)\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return MultiEpochsDataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "            persistent_workers=(self.hparams.num_workers > 0)\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return MultiEpochsDataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "            persistent_workers=(self.hparams.num_workers > 0)\n",
    "        )\n",
    "\n",
    "    def all_dataloader(self):\n",
    "        return MultiEpochsDataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=True,\n",
    "            persistent_workers=(self.hparams.num_workers > 0)\n",
    "        )\n",
    "\n",
    "class MultiEpochsDataLoader(torch.utils.data.DataLoader):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._DataLoader__initialized = False\n",
    "        self.batch_sampler = _RepeatSampler(self.batch_sampler)\n",
    "        self._DataLoader__initialized = True\n",
    "        self.iterator = super().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler.sampler)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield next(self.iterator)\n",
    "\n",
    "\n",
    "class _RepeatSampler(object):\n",
    "    \"\"\" Sampler that repeats forever.\n",
    "    Args:\n",
    "        sampler (Sampler)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield from iter(self.sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca3717bbff9d009",
   "metadata": {},
   "source": [
    "### U-Net Model\n",
    "\n",
    "The class below defines the U-Net architecture using `pl.LightningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b212177329a1370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class plUNET(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            lr: float = 0.001,\n",
    "            weight_decay: float = 0.0005,\n",
    "            num_channels: int = 1,\n",
    "            loss='dice'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.net = smp.UnetPlusPlus(encoder_name='resnet50', in_channels=num_channels, classes=2)\n",
    "\n",
    "        if loss == 'dice':\n",
    "            self.criterion = smp.losses.DiceLoss(mode='multiclass')\n",
    "        elif loss == 'ce':\n",
    "            self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_auc = AUROC(task=\"binary\")\n",
    "        self.train_f1 = F1Score(task=\"binary\")\n",
    "        self.train_auprc = AveragePrecision(task=\"binary\")\n",
    "        self.val_auc = AUROC(task=\"binary\")\n",
    "        self.val_f1 = F1Score(task=\"binary\")\n",
    "        self.val_auprc = AveragePrecision(task=\"binary\")\n",
    "        self.test_auc = AUROC(task=\"binary\")\n",
    "        self.test_auprc = AveragePrecision(task=\"binary\")\n",
    "        self.test_f1 = F1Score(task=\"binary\")\n",
    "\n",
    "        self.train_positive_count = 0\n",
    "        self.val_positive_count = 0\n",
    "        self.test_positive_count = 0\n",
    "        self.train_negative_count = 0\n",
    "        self.val_negative_count = 0\n",
    "        self.test_negative_count = 0\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "    def step(self, batch: Any):\n",
    "        x, y = batch\n",
    "        y = y.long()\n",
    "        logits = self.forward(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        preds = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        return loss, preds, y, x\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets, inputs = self.step(batch)\n",
    "        self.train_auc.update(preds, targets)\n",
    "        self.train_auprc.update(preds.flatten(), targets.flatten())\n",
    "        self.train_f1.update(preds.flatten(), targets.flatten())\n",
    "\n",
    "        self.log(\"train/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train/auc\", self.train_auc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/auprc\", self.train_auprc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"train/f1\", self.train_f1, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets, \"inputs\": inputs}\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets, _ = self.step(batch)\n",
    "\n",
    "        # log val metrics\n",
    "        self.val_auc.update(preds, targets)\n",
    "        self.val_auprc.update(preds.flatten(), targets.flatten())\n",
    "        self.val_f1.update(preds.flatten(), targets.flatten())\n",
    "\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/auc\", self.val_auc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/auprc\", self.val_auprc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/f1\", self.val_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        loss, preds, targets, _ = self.step(batch)\n",
    "        self.test_auc.update(preds, targets)\n",
    "        self.test_auprc.update(preds.flatten(), targets.flatten())\n",
    "        self.test_f1.update(preds.flatten(), targets.flatten())\n",
    "\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test/auc\", self.test_auc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"test/auprc\", self.test_auprc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"test/f1\", self.test_f1, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler, \"monitor\": \"train/loss\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956c11c",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d75fd-c9e6-4915-aecb-b41c7f9edaa8",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300b42d-211a-4fa7-8b8b-1eca20f3f026",
   "metadata": {},
   "source": [
    "The input data can be foun in a compressed file `hokkaido_japan.zip` in [Zenodo](https://doi.org/10.5281/zenodo.7248056). e use`pooch` to fetch and unzip them locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76785bd1-52ad-42cb-9cce-8001dc977495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooch.retrieve(\n",
    "    url=\"doi:10.5281/zenodo.7248056/hokkaido_japan.zip\",\n",
    "    known_hash=\"md5:699b94e827c72bcd69bd786e56bfe5dc\",\n",
    "    processor=pooch.Unzip(extract_dir='data'),\n",
    "    path=f\".\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43e9a6ea1f055b",
   "metadata": {},
   "source": [
    "### Define the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013c638dc28b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = BeforeAfterCubeDataModule(\n",
    "    ds_path=hparams[\"ds_path\"],\n",
    "    ba_vars=['vv', 'vh'],\n",
    "    aggregation='mean',\n",
    "    timestep_length=hparams[\"timestep_length\"],\n",
    "    event_start_date='20180905',\n",
    "    event_end_date='20180907',\n",
    "    input_vars=hparams[\"input_vars\"].split(','),\n",
    "    target='landslides',\n",
    "    train_val_test_split=(0.7, 0.2, 0.1),\n",
    "    batch_size=hparams[\"batch_size\"],\n",
    "    include_negatives=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e5f767e67664b",
   "metadata": {},
   "source": [
    "### Initiate the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e38ad745b14732",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921d51be43de425",
   "metadata": {},
   "source": [
    "## U-Net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca623f6872a0b4b3",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbcd588eb01c432",
   "metadata": {},
   "source": [
    "We start defining the model using the `plUNET` class and the hyperparameters defined in the `hparams` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e265ae6130f54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = plUNET(\n",
    "        lr=hparams[\"lr\"],\n",
    "        weight_decay=hparams[\"weight_decay\"],\n",
    "        num_channels=len(hparams[\"input_vars\"]),\n",
    "        loss=hparams[\"loss\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89934db9bca074d8",
   "metadata": {},
   "source": [
    "Then we initiate the trainer with the maximum number of epochs defined in the `hparams` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99c8eb1061be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=hparams[\"max_epochs\"], accelerator=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be36a24aad32710",
   "metadata": {},
   "source": [
    "Finally, we fit the trainer with the model and the datacube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd183cef5ecc8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275521b39a74000",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc8d0f4cfed7c7",
   "metadata": {},
   "source": [
    "We assess the model performance using the test dataset and the best model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03327acd-6a46-481e-9025-6e94e6d52a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.test(ckpt_path=\"best\", datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38650c",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b2ecd-eed4-437a-b27c-8850791fc2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dm.test_dataloader():\n",
    "    input = i[0]\n",
    "    target = i[1]\n",
    "    output = model(input)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1475fba-d335-48cc-8566-5370a0169a41",
   "metadata": {},
   "source": [
    " ## References\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"landslides\" and not topic % \"package\"\n",
    ":keyprefix: a-\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf3c7c-6684-4edf-99cc-e30016b2d4a8",
   "metadata": {},
   "source": [
    "## Packages citation\n",
    "\n",
    "```{bibliography}\n",
    ":style: alpha\n",
    ":filter: topic % \"package\"\n",
    ":keyprefix: a-\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
